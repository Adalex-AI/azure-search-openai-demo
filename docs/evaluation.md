# Evaluating the RAG answer quality

[üì∫ Watch: (RAG Deep Dive series) Evaluating RAG answer quality](https://www.youtube.com/watch?v=lyCLu53fb3g)

Follow these steps to evaluate the quality of the answers generated by the RAG flow.

* [Deploy an evaluation model](#deploy-an-evaluation-model)
* [Setup the evaluation environment](#setup-the-evaluation-environment)
* [Generate ground truth data](#generate-ground-truth-data)
* [Understanding the evaluation metrics](#understanding-the-evaluation-metrics)
* [Run bulk evaluation](#run-bulk-evaluation)
* [Review the evaluation results](#review-the-evaluation-results)
* [Run bulk evaluation on a PR](#run-bulk-evaluation-on-a-pr)

## Deploy an evaluation model

1. Run this command to tell `azd` to deploy a GPT-4 level model for evaluation:

    ```shell
    azd env set USE_EVAL true
    ```

2. Set the capacity to the highest possible value to ensure that the evaluation runs relatively quickly. Even with a high capacity, it can take a long time to generate ground truth data and run bulk evaluations.

    ```shell
    azd env set AZURE_OPENAI_EVAL_DEPLOYMENT_CAPACITY 100
    ```

    By default, that will provision a `gpt-4o` model, version `2024-08-06`. To change those settings, set the azd environment variables `AZURE_OPENAI_EVAL_MODEL` and `AZURE_OPENAI_EVAL_MODEL_VERSION` to the desired values.

3. Then, run the following command to provision the model:

    ```shell
    azd provision
    ```

## Setup the evaluation environment

Make a new Python virtual environment and activate it. This is currently required due to incompatibilities between the dependencies of the evaluation script and the main project.

```bash
python -m venv .evalenv
```

```bash
source .evalenv/bin/activate
```

Install all the dependencies for the evaluation script by running the following command:

```bash
pip install -r evals/requirements.txt
```

## Generate ground truth data

Generate ground truth data by running the following command:

```bash
python evals/generate_ground_truth.py --numquestions=200 --numsearchdocs=1000
```

The options are:

* `numquestions`: The number of questions to generate. We suggest at least 200.
* `numsearchdocs`: The number of documents (chunks) to retrieve from your search index. You can leave off the option to fetch all documents, but that will significantly increase time it takes to generate ground truth data. You may want to at least start with a subset.
* `kgfile`: An existing RAGAS knowledge base JSON file, which is usually `ground_truth_kg.json`. You may want to specify this if you already created a knowledge base and just want to tweak the question generation steps.
* `groundtruthfile`: The file to write the generated ground truth answers. By default, this is `evals/ground_truth.jsonl`.

üï∞Ô∏è This may take a long time, possibly several hours, depending on the size of the search index.

**Important for Legal RAG Systems**: The generated questions should reflect realistic queries that legal professionals would ask. Review the practical test questions in `evals/practical_test_questions.json` for examples of:
- Pre-action protocol questions (letter before claim requirements, deadlines)
- Limitation period calculations
- Case management conference requirements
- Court-specific procedures (Commercial Court, Circuit Commercial Court, etc.)
- Disclosure obligations
- Time-sensitive procedural deadlines

Review the generated data in `evals/ground_truth.jsonl` after running that script, removing any question/answer pairs that don't seem like realistic user input. Ensure questions are practice-focused rather than academic.

## Understanding the evaluation metrics

This evaluation framework includes both standard RAG metrics and custom legal-specific metrics:

### Standard Metrics

* **gpt_groundedness**: GPT-4 evaluates whether the answer is grounded in the source documents
* **gpt_relevance**: GPT-4 evaluates whether the answer is relevant to the question
* **answer_length**: Measures the length of the generated answer
* **latency**: Measures how long it takes to generate an answer

### Legal Citation Metrics

#### any_citation
Checks if the response contains **any** citations in the correct `[1]`, `[2]`, `[3]` format.

**Pass criteria**: At least one citation present in the format `[N]` where N is a number.

#### citations_matched
Verifies that citations in the response match those in the ground truth.

**Scoring**: Percentage of ground truth citations that appear in the response.
- Score of 1.0 = All expected citations present
- Score of 0.5 = Half of expected citations present
- Score of 0.0 = No expected citations present

#### citation_format_compliance
Validates that citations follow the **strict** format rules defined in `CITATION_FORMAT_FIXES.md`.

**Prohibited patterns detected**:
- `[1, 2, 3]` - Commas inside brackets
- `1, 2, [3]` - Mixed bracketed and unbracketed
- `[1], [2]` - Commas between citations (should be `[1][2]`)
- `1, 2, 3` - Naked comma-separated numbers

**Pass criteria**: No prohibited patterns detected (score = 1.0), any violation = 0.0

#### subsection_extraction_accuracy
Validates that legal paragraph/subsection numbers mentioned in responses actually exist in the cited source documents.

**What it checks**:
- Extracts subsection references: `31.1`, `A4.1`, `PD 3E-1.1`, `CPR Part 31.6`
- Verifies each extracted reference appears in at least one source document
- Prevents hallucinated subsection numbers

**Scoring**: Percentage of mentioned subsections that exist in sources
- Score of 1.0 = All mentioned subsections verified in sources
- Score of 0.0 = Mentioned subsections not found in sources (hallucination)

#### category_coverage
Ensures the system retrieves documents from the correct court categories based on the query.

**Categories tested**:
- Civil Procedure Rules
- Commercial Court
- Circuit Commercial Court
- King's Bench Division
- High Court
- County Court
- Chancery
- Patents Court
- Technology and Construction Court

**Pass criteria**:
- If query mentions a specific court: sources must include that court OR Civil Procedure Rules (fallback)
- If no specific court mentioned: any categorized sources are acceptable

**Example**: Query asks about "Circuit Commercial Court CMC requirements" ‚Üí sources should include Circuit Commercial Court Guide and/or CPR documents.

## Run bulk evaluation

Review the configuration in `evals/evaluate_config.json` to ensure that everything is correctly setup. You may want to adjust the metrics used. See [the ai-rag-chat-evaluator README](https://github.com/Azure-Samples/ai-rag-chat-evaluator) for more information on the available metrics.

By default, the evaluation script will evaluate every question in the ground truth data.
Run the evaluation script by running the following command:

```bash
python evals/evaluate.py
```

The options are:

* `numquestions`: The number of questions to evaluate. By default, this is all questions in the ground truth data.
* `resultsdir`: The directory to write the evaluation results. By default, this is a timestamped folder in `evals/results`. This option can also be specified in `eval_config.json`.
* `targeturl`: The URL of the running application to evaluate. By default, this is `http://localhost:50505`. This option can also be specified in `eval_config.json`.

üï∞Ô∏è This may take a long time, possibly several hours, depending on the number of ground truth questions, and the TPM capacity of the evaluation model, and the number of GPT metrics requested.

### Quick Testing

To test the evaluation pipeline with a small sample before running a full evaluation:

```bash
python evals/evaluate.py --numquestions=10
```

This will evaluate only the first 10 questions and is useful for:
- Verifying metrics are working correctly
- Testing after prompt changes
- Quick sanity checks before full evaluation runs

## Review the evaluation results

The evaluation script will output a summary of the evaluation results, inside the `evals/results` directory.

You can see a summary of results across all evaluation runs by running the following command:

```bash
python -m evaltools summary evals/results
```

Compare answers to the ground truth by running the following command:

```bash
python -m evaltools diff evals/results/baseline/
```

Compare answers across two runs by running the following command:

```bash
python -m evaltools diff evals/results/baseline/ evals/results/SECONDRUNHERE
```

## Run bulk evaluation on a PR

This repository includes a GitHub Action workflow `evaluate.yaml` that can be used to run the evaluation on the changes in a PR.

In order for the workflow to run successfully, you must first set up [continuous integration](./azd.md#github-actions) for the repository.

To run the evaluation on the changes in a PR, a repository member can post a `/evaluate` comment to the PR. This will trigger the evaluation workflow to run the evaluation on the PR changes and will post the results to the PR.
