name: Legal Document Scraper Pipeline

on:
  schedule:
    - cron: '0 0 * * 0'  # Weekly on Sunday at midnight
  workflow_dispatch:
    inputs:
      dry_run:
        description: 'Run in dry-run mode (no upload)'
        required: false
        type: boolean
        default: false
      force_upload:
        description: 'Force upload even if validation fails (NOT RECOMMENDED)'
        required: false
        type: boolean
        default: false

permissions:
  id-token: write
  contents: read

concurrency:
  group: legal-scraper-${{ github.ref }}
  cancel-in-progress: true

jobs:
  scrape-and-validate:
    runs-on: ubuntu-latest
    outputs:
      validation_status: ${{ steps.validate.outputs.status }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
          cache-dependency-path: 'requirements-dev.txt'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt

      - name: Install Chrome
        uses: browser-actions/setup-chrome@latest

      - name: Run Scraper
        run: |
          python scripts/legal-scraper/scrape_cpr.py
        env:
          # Scraper might need these if it does any embedding generation during scrape (unlikely but safe to add)
          AZURE_OPENAI_SERVICE: ${{ secrets.AZURE_OPENAI_SERVICE }}
          AZURE_OPENAI_EMB_DEPLOYMENT: ${{ secrets.AZURE_OPENAI_EMB_DEPLOYMENT }}

      - name: Run Validation
        id: validate
        run: |
          if python scripts/legal-scraper/validate_and_review.py --input Upload; then
            echo "status=success" >> $GITHUB_OUTPUT
          else
            echo "status=failed" >> $GITHUB_OUTPUT
            exit 1
          fi
        continue-on-error: true 

      - name: Generate Upload Plan (Dry Run)
        run: |
          echo "Generating upload plan (Diff Analysis)..."
          # Attempt to run dry-run to show changes. Will skip if secrets are missing.
          if [[ -n "${{ secrets.AZURE_SEARCH_KEY }}" || -n "${{ secrets.AZURE_SEARCH_SERVICE }}" ]]; then
            python scripts/legal-scraper/upload_with_embeddings.py --input Upload --dry-run
          else
            echo "⚠️  Skipping Upload Plan: Azure secrets not available in this job context."
          fi
        continue-on-error: true
        env:
          AZURE_SEARCH_SERVICE: ${{ secrets.AZURE_SEARCH_SERVICE }}
          AZURE_SEARCH_INDEX: ${{ secrets.AZURE_SEARCH_INDEX }}
          AZURE_SEARCH_KEY: ${{ secrets.AZURE_SEARCH_KEY }}
          AZURE_OPENAI_SERVICE: ${{ secrets.AZURE_OPENAI_SERVICE }}
          AZURE_OPENAI_KEY: ${{ secrets.AZURE_OPENAI_KEY }}
          AZURE_OPENAI_EMB_DEPLOYMENT: ${{ secrets.AZURE_OPENAI_EMB_DEPLOYMENT }}

      - name: Upload Scraped Data
        uses: actions/upload-artifact@v4
        with:
          name: scraped-data
          path: data/legal-scraper/processed/Upload/
          retention-days: 7

      - name: Upload Validation Reports
        uses: actions/upload-artifact@v4
        with:
          name: validation-reports
          path: data/legal-scraper/reports/
          retention-days: 7

  upload-production:
    needs: scrape-and-validate
    runs-on: ubuntu-latest
    environment: Production
    if: ${{ github.event.inputs.dry_run != 'true' && (needs.scrape-and-validate.outputs.validation_status == 'success' || github.event.inputs.force_upload == 'true') }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
          cache-dependency-path: 'requirements-dev.txt'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt

      - name: Download Scraped Data
        uses: actions/download-artifact@v4
        with:
          name: scraped-data
          path: data/legal-scraper/processed/Upload/

      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Upload to Azure Search
        uses: nick-fields/retry@v3
        with:
          timeout_minutes: 30
          max_attempts: 3
          retry_wait_seconds: 60
          command: python scripts/legal-scraper/upload_with_embeddings.py --input Upload
        env:
          AZURE_SEARCH_SERVICE: ${{ secrets.AZURE_SEARCH_SERVICE }}
          AZURE_SEARCH_INDEX: ${{ secrets.AZURE_SEARCH_INDEX }}
          AZURE_SEARCH_KEY: ${{ secrets.AZURE_SEARCH_KEY }}
          AZURE_OPENAI_SERVICE: ${{ secrets.AZURE_OPENAI_SERVICE }}
          AZURE_OPENAI_KEY: ${{ secrets.AZURE_OPENAI_KEY }}
          AZURE_OPENAI_EMB_DEPLOYMENT: ${{ secrets.AZURE_OPENAI_EMB_DEPLOYMENT }}
          AZURE_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}

      # Optional: Slack Notification
      # - name: Slack Notification
      #   uses: slackapi/slack-github-action@v1.24.0
      #   if: always()
      #   with:
      #     payload: |
      #       {
      #         "text": "Legal Scraper Pipeline Finished\nStatus: ${{ job.status }}"
      #       }
      #   env:
      #     SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
